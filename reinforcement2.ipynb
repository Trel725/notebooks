{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "import logging\n",
    "import sneks\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits, **kwargs):\n",
    "        # Sample a random categorical action from the given logits.\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__('mlp_policy')\n",
    "        # Note: no tf.get_variable(), just simple Keras API!\n",
    "        self.hidden1 = kl.Dense(128, activation='relu')\n",
    "        self.hidden2 = kl.Dense(128, activation='relu')\n",
    "        self.value = kl.Dense(1, name='value')\n",
    "        # Logits are unnormalized log probabilities.\n",
    "        self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "        self.dist = ProbabilityDistribution()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Inputs is a numpy array, convert to a tensor.\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        # Separate hidden layers from the same input tensor.\n",
    "        hidden_logs = self.hidden1(x)\n",
    "        hidden_vals = self.hidden2(x)\n",
    "        return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "    def action_value(self, obs):\n",
    "        # Executes `call()` under the hood.\n",
    "        logits, value = self.predict_on_batch(obs)\n",
    "        action = self.dist.predict_on_batch(logits)\n",
    "        # Another way to sample actions:\n",
    "        #   action = tf.random.categorical(logits, 1)\n",
    "        # Will become clearer later why we don't use it.\n",
    "        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -0.0042053466\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "\n",
    "obs = env.reset()\n",
    "# No feed_dict or tf.Session() needed at all!\n",
    "action, value = model.action_value(obs[None, :])\n",
    "print(action, value) # [1] [-0.00145713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "        self.value_c = value_c\n",
    "        self.entropy_c = entropy_c\n",
    "\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "            optimizer=ko.RMSprop(lr=lr),\n",
    "            # Define separate losses for policy logits and value estimate.\n",
    "            loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "    def test(self, env, render=True):\n",
    "        obs, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(obs[None, :])\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "        return ep_reward\n",
    "\n",
    "    def train(self, env, batch_sz=64, updates=250):\n",
    "        # Storage helpers for a single batch of data.\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "\n",
    "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = env.reset()\n",
    "        for update in range(updates):\n",
    "            for step in range(batch_sz):\n",
    "                observations[step] = next_obs.copy()\n",
    "                actions[step], values[step] = self.model.action_value(\n",
    "                    next_obs[None, :])\n",
    "                next_obs, rewards[step], dones[step], _ = env.step(\n",
    "                    actions[step])\n",
    "\n",
    "                ep_rewards[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rewards.append(0.0)\n",
    "                    next_obs = env.reset()\n",
    "                    print(\"Episode: %03d, Reward: %03d\" % (\n",
    "                        len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "            _, next_value = self.model.action_value(next_obs[None, :])\n",
    "\n",
    "            returns, advs = self._returns_advantages(\n",
    "                rewards, dones, values, next_value)\n",
    "            # A trick to input actions and advantages through same API.\n",
    "            acts_and_advs = np.concatenate(\n",
    "                [actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "            # Performs a full training step on the collected batch.\n",
    "            # Note: no need to mess around with gradients, Keras API handles it.\n",
    "            losses = self.model.train_on_batch(\n",
    "                observations, [acts_and_advs, returns])\n",
    "\n",
    "            print(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "        returns = np.append(np.zeros_like(rewards), next_value)\n",
    "\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * \\\n",
    "                returns[t + 1] * (1 - dones[t])\n",
    "        returns = returns[:-1]\n",
    "\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "    def _value_loss(self, returns, value):\n",
    "        # Value loss is typically MSE between value estimates and returns.\n",
    "        return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "    def _logits_loss(self, actions_and_advantages, logits):\n",
    "        # A trick to input actions and advantages through the same API.\n",
    "        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "\n",
    "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "\n",
    "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "        # Note: we only calculate the loss on the actions we've actually taken.\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(\n",
    "            actions, logits, sample_weight=advantages)\n",
    "\n",
    "        # Entropy loss can be calculated as cross-entropy over itself.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "\n",
    "        # We want to minimize policy and maximize entropy losses.\n",
    "        # Here signs are flipped because the optimizer minimizes.\n",
    "        return policy_loss - self.entropy_c * entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 out of 200\n"
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(model)\n",
    "rewards_sum = agent.test(env)\n",
    "print(\"%d out of 200\" % rewards_sum) # 18 out of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001, Reward: 020\n",
      "Episode: 002, Reward: 013\n",
      "Episode: 003, Reward: 012\n",
      "[1/250] Losses: [52.647274, 5.864591, 46.782684]\n",
      "Episode: 004, Reward: 022\n",
      "Episode: 005, Reward: 032\n",
      "Episode: 006, Reward: 012\n",
      "Episode: 007, Reward: 008\n",
      "[2/250] Losses: [82.357864, 6.4930053, 75.86486]\n",
      "Episode: 008, Reward: 040\n",
      "[3/250] Losses: [147.3424, 9.925774, 137.41663]\n",
      "Episode: 009, Reward: 064\n",
      "[4/250] Losses: [149.2958, 9.561987, 139.73383]\n",
      "Episode: 010, Reward: 059\n",
      "[5/250] Losses: [153.56667, 9.253948, 144.31271]\n",
      "[6/250] Losses: [443.7147, 16.368816, 427.3459]\n",
      "[7/250] Losses: [438.23413, 15.093183, 423.14096]\n",
      "Episode: 011, Reward: 181\n",
      "Episode: 012, Reward: 045\n",
      "[8/250] Losses: [161.67554, 8.616213, 153.05933]\n",
      "[9/250] Losses: [438.55112, 14.438739, 424.11237]\n",
      "Episode: 013, Reward: 104\n",
      "[10/250] Losses: [122.97572, 7.4709663, 115.50476]\n",
      "Episode: 014, Reward: 059\n",
      "[11/250] Losses: [125.731255, 7.3567414, 118.37451]\n",
      "Episode: 015, Reward: 042\n",
      "[12/250] Losses: [308.8355, 11.286303, 297.5492]\n",
      "Episode: 016, Reward: 062\n",
      "[13/250] Losses: [319.36133, 11.939765, 307.42157]\n",
      "[14/250] Losses: [419.07877, 13.5174885, 405.56128]\n",
      "Episode: 017, Reward: 167\n",
      "[15/250] Losses: [151.11032, 7.0377216, 144.0726]\n",
      "[16/250] Losses: [413.07327, 13.919087, 399.15417]\n",
      "Episode: 018, Reward: 101\n",
      "[17/250] Losses: [171.0892, 7.6281304, 163.46107]\n",
      "Episode: 019, Reward: 062\n",
      "Episode: 020, Reward: 031\n",
      "[18/250] Losses: [48.275414, 4.020705, 44.254707]\n",
      "Episode: 021, Reward: 036\n",
      "Episode: 022, Reward: 034\n",
      "[19/250] Losses: [55.248814, 4.197734, 51.05108]\n",
      "Episode: 023, Reward: 036\n",
      "Episode: 024, Reward: 033\n",
      "[20/250] Losses: [57.878296, 3.659315, 54.218983]\n",
      "Episode: 025, Reward: 040\n",
      "[21/250] Losses: [102.28318, 6.554624, 95.72855]\n",
      "[22/250] Losses: [385.31024, 13.814995, 371.49524]\n",
      "Episode: 026, Reward: 101\n",
      "Episode: 027, Reward: 034\n",
      "[23/250] Losses: [61.763084, 4.591609, 57.171474]\n",
      "Episode: 028, Reward: 050\n",
      "[24/250] Losses: [107.23928, 6.519811, 100.71947]\n",
      "Episode: 029, Reward: 061\n",
      "Episode: 030, Reward: 039\n",
      "[25/250] Losses: [67.38182, 3.7055776, 63.676247]\n",
      "Episode: 031, Reward: 039\n",
      "[26/250] Losses: [84.50084, 6.2243176, 78.27652]\n",
      "Episode: 032, Reward: 043\n",
      "[27/250] Losses: [177.47612, 9.055979, 168.42014]\n",
      "Episode: 033, Reward: 106\n",
      "[28/250] Losses: [215.5917, 9.759804, 205.83191]\n",
      "[29/250] Losses: [444.3966, 16.684189, 427.71243]\n",
      "Episode: 034, Reward: 094\n",
      "[30/250] Losses: [123.07596, 5.643844, 117.43211]\n",
      "[31/250] Losses: [385.76422, 14.104744, 371.6595]\n",
      "[32/250] Losses: [412.55643, 15.802837, 396.7536]\n",
      "Episode: 035, Reward: 174\n",
      "Episode: 036, Reward: 047\n",
      "[33/250] Losses: [96.33211, 2.5745683, 93.75754]\n",
      "[34/250] Losses: [453.50912, 15.855798, 437.65332]\n",
      "Episode: 037, Reward: 103\n",
      "[35/250] Losses: [80.5923, 3.7670326, 76.825264]\n",
      "[36/250] Losses: [354.82547, 14.059292, 340.76617]\n",
      "Episode: 038, Reward: 113\n",
      "[37/250] Losses: [190.57875, 7.418071, 183.16068]\n",
      "Episode: 039, Reward: 058\n",
      "[38/250] Losses: [215.30142, 8.672914, 206.62851]\n",
      "[39/250] Losses: [361.1626, 13.180685, 347.9819]\n",
      "[40/250] Losses: [339.70834, 12.706297, 327.00204]\n",
      "Episode: 040, Reward: 191\n",
      "[41/250] Losses: [229.05536, 8.312405, 220.74295]\n",
      "[42/250] Losses: [371.67374, 13.680617, 357.99313]\n",
      "Episode: 041, Reward: 148\n",
      "[43/250] Losses: [96.64838, 2.499729, 94.14865]\n",
      "Episode: 042, Reward: 065\n",
      "[44/250] Losses: [73.8467, 2.6762738, 71.170425]\n",
      "[45/250] Losses: [333.7709, 12.358387, 321.4125]\n",
      "Episode: 043, Reward: 143\n",
      "[46/250] Losses: [59.879745, 1.0098059, 58.869938]\n",
      "Episode: 044, Reward: 043\n",
      "[47/250] Losses: [117.12046, 3.8950238, 113.22543]\n",
      "[48/250] Losses: [318.1388, 12.21905, 305.91974]\n",
      "[49/250] Losses: [295.99506, 12.126631, 283.86844]\n",
      "Episode: 045, Reward: 200\n",
      "[50/250] Losses: [80.145515, 1.9313346, 78.21418]\n",
      "[51/250] Losses: [301.70172, 13.821179, 287.88055]\n",
      "Episode: 046, Reward: 157\n",
      "[52/250] Losses: [106.820366, -0.3112564, 107.13162]\n",
      "Episode: 047, Reward: 034\n",
      "[53/250] Losses: [93.38224, 1.9763104, 91.40593]\n",
      "[54/250] Losses: [322.2007, 12.166867, 310.03384]\n",
      "Episode: 048, Reward: 100\n",
      "[55/250] Losses: [266.9074, 11.63987, 255.26753]\n",
      "[56/250] Losses: [273.15814, 12.191585, 260.96655]\n",
      "[57/250] Losses: [262.30948, 11.577675, 250.7318]\n",
      "Episode: 049, Reward: 200\n",
      "Episode: 050, Reward: 022\n",
      "[58/250] Losses: [106.157326, -1.23316, 107.39049]\n",
      "[59/250] Losses: [220.8307, 10.295557, 210.53514]\n",
      "[60/250] Losses: [342.9346, 12.789507, 330.14508]\n",
      "Episode: 051, Reward: 193\n",
      "[61/250] Losses: [79.792496, -0.5284764, 80.32097]\n",
      "[62/250] Losses: [267.59854, 11.126494, 256.47205]\n",
      "[63/250] Losses: [240.42206, 10.611955, 229.8101]\n",
      "Episode: 052, Reward: 200\n",
      "[64/250] Losses: [73.823425, -2.2961743, 76.1196]\n",
      "[65/250] Losses: [252.33759, 10.580071, 241.7575]\n",
      "[66/250] Losses: [212.8271, 9.430101, 203.397]\n",
      "Episode: 053, Reward: 200\n",
      "[67/250] Losses: [60.424877, -2.3466973, 62.771576]\n",
      "[68/250] Losses: [203.02669, 9.798823, 193.22786]\n",
      "[69/250] Losses: [204.54901, 10.034142, 194.51486]\n",
      "Episode: 054, Reward: 200\n",
      "[70/250] Losses: [88.58384, -3.7914362, 92.375275]\n",
      "[71/250] Losses: [215.64467, 10.442638, 205.20203]\n",
      "[72/250] Losses: [246.82002, 10.7214, 236.09862]\n",
      "[73/250] Losses: [176.64282, 9.827965, 166.81485]\n",
      "Episode: 055, Reward: 200\n",
      "[74/250] Losses: [186.39995, 8.989753, 177.4102]\n",
      "[75/250] Losses: [190.03995, 8.984703, 181.05524]\n",
      "Episode: 056, Reward: 171\n",
      "[76/250] Losses: [124.15076, -5.2260556, 129.37682]\n",
      "[77/250] Losses: [186.98422, 8.973642, 178.01057]\n",
      "[78/250] Losses: [196.1703, 9.13189, 187.0384]\n",
      "Episode: 057, Reward: 200\n",
      "[79/250] Losses: [130.74086, -5.5040126, 136.24487]\n",
      "[80/250] Losses: [222.4259, 10.307075, 212.11884]\n",
      "Episode: 058, Reward: 135\n",
      "[81/250] Losses: [124.50791, -4.5432844, 129.0512]\n",
      "[82/250] Losses: [162.46484, 8.861891, 153.60295]\n",
      "Episode: 059, Reward: 102\n",
      "[83/250] Losses: [118.753654, -3.6169481, 122.370605]\n",
      "[84/250] Losses: [223.0268, 10.409747, 212.61705]\n",
      "Episode: 060, Reward: 132\n",
      "[85/250] Losses: [131.1406, -4.2735763, 135.41417]\n",
      "[86/250] Losses: [160.11095, 8.887996, 151.22295]\n",
      "Episode: 061, Reward: 119\n",
      "[87/250] Losses: [122.58503, -2.114634, 124.69966]\n",
      "[88/250] Losses: [200.23878, 10.122779, 190.11601]\n",
      "Episode: 062, Reward: 112\n",
      "[89/250] Losses: [148.73524, 2.7077258, 146.02751]\n",
      "[90/250] Losses: [179.03499, 10.311728, 168.72327]\n",
      "Episode: 063, Reward: 150\n",
      "[91/250] Losses: [144.61555, -3.9354897, 148.55104]\n",
      "[92/250] Losses: [155.62134, 9.391531, 146.22981]\n",
      "[93/250] Losses: [196.49074, 9.653791, 186.83694]\n",
      "Episode: 064, Reward: 200\n",
      "[94/250] Losses: [165.10953, -6.8951674, 172.0047]\n",
      "[95/250] Losses: [146.66539, 9.087133, 137.57826]\n",
      "[96/250] Losses: [125.42809, 6.833673, 118.59442]\n",
      "Episode: 065, Reward: 200\n",
      "[97/250] Losses: [217.28638, -8.633736, 225.92012]\n",
      "[98/250] Losses: [123.18768, 8.718418, 114.46927]\n",
      "[99/250] Losses: [219.3702, 10.96295, 208.40724]\n",
      "Episode: 066, Reward: 200\n",
      "[100/250] Losses: [230.65158, -9.737038, 240.38863]\n",
      "[101/250] Losses: [138.67409, 8.149707, 130.52438]\n",
      "[102/250] Losses: [165.55643, 9.606055, 155.95038]\n",
      "[103/250] Losses: [141.47697, 9.035114, 132.44186]\n",
      "Episode: 067, Reward: 200\n",
      "[104/250] Losses: [144.41064, 7.492845, 136.9178]\n",
      "[105/250] Losses: [147.94972, 8.502196, 139.44753]\n",
      "[106/250] Losses: [150.90976, 8.923385, 141.98637]\n",
      "Episode: 068, Reward: 200\n",
      "[107/250] Losses: [231.459, 1.9440666, 229.51494]\n",
      "[108/250] Losses: [139.48169, 9.097681, 130.384]\n",
      "Episode: 069, Reward: 178\n",
      "[109/250] Losses: [371.95032, -14.395253, 386.34558]\n",
      "[110/250] Losses: [118.93691, 7.3940096, 111.5429]\n",
      "[111/250] Losses: [115.34129, 7.2555885, 108.08571]\n",
      "[112/250] Losses: [117.25419, 7.438384, 109.8158]\n",
      "Episode: 070, Reward: 200\n",
      "[113/250] Losses: [172.9925, 5.469702, 167.5228]\n",
      "[114/250] Losses: [122.71082, 7.7857614, 114.925064]\n",
      "[115/250] Losses: [99.18973, 6.769052, 92.42068]\n",
      "Episode: 071, Reward: 200\n",
      "[116/250] Losses: [265.7076, -0.13972345, 265.84732]\n",
      "[117/250] Losses: [91.51109, 6.301215, 85.20988]\n",
      "[118/250] Losses: [111.329796, 7.31708, 104.01272]\n",
      "Episode: 072, Reward: 200\n",
      "[119/250] Losses: [377.09857, -5.7736406, 382.87222]\n",
      "Episode: 073, Reward: 050\n",
      "[120/250] Losses: [206.10088, 3.1982138, 202.90266]\n",
      "[121/250] Losses: [95.535576, 6.791319, 88.744255]\n",
      "[122/250] Losses: [92.38953, 6.4286633, 85.96086]\n",
      "Episode: 074, Reward: 200\n",
      "[123/250] Losses: [320.0786, -1.2257714, 321.30438]\n",
      "[124/250] Losses: [88.93355, 6.489499, 82.444046]\n",
      "Episode: 075, Reward: 154\n",
      "[125/250] Losses: [508.69504, -15.9661045, 524.66113]\n",
      "[126/250] Losses: [96.17045, 6.7184086, 89.45204]\n",
      "[127/250] Losses: [109.64811, 7.50574, 102.14237]\n",
      "Episode: 076, Reward: 200\n",
      "[128/250] Losses: [551.70886, -15.757828, 567.4667]\n",
      "Episode: 077, Reward: 028\n",
      "[129/250] Losses: [289.1447, -4.78646, 293.93118]\n",
      "[130/250] Losses: [91.79486, 6.8375807, 84.95728]\n",
      "[131/250] Losses: [91.27291, 6.826041, 84.44687]\n",
      "Episode: 078, Reward: 200\n",
      "[132/250] Losses: [385.54745, -4.87121, 390.41867]\n",
      "[133/250] Losses: [101.08645, 7.1056204, 93.98083]\n",
      "[134/250] Losses: [83.01825, 6.4423437, 76.575905]\n",
      "Episode: 079, Reward: 200\n",
      "[135/250] Losses: [472.714, -11.502362, 484.21634]\n",
      "[136/250] Losses: [78.235245, 6.1038322, 72.13142]\n",
      "[137/250] Losses: [83.74162, 6.155468, 77.58615]\n",
      "Episode: 080, Reward: 200\n",
      "[138/250] Losses: [544.1211, -12.065993, 556.1871]\n",
      "[139/250] Losses: [75.87378, 6.7550907, 69.11869]\n",
      "[140/250] Losses: [83.45839, 6.2780113, 77.180374]\n",
      "Episode: 081, Reward: 200\n",
      "[141/250] Losses: [562.4272, -16.366411, 578.7936]\n",
      "[142/250] Losses: [79.42862, 5.7048054, 73.723816]\n",
      "Episode: 082, Reward: 147\n",
      "[143/250] Losses: [541.66943, -17.282646, 558.9521]\n",
      "[144/250] Losses: [93.395226, 6.392707, 87.00252]\n",
      "[145/250] Losses: [97.65166, 7.3729286, 90.27873]\n",
      "[146/250] Losses: [82.68976, 6.187752, 76.50201]\n",
      "Episode: 083, Reward: 200\n",
      "[147/250] Losses: [232.73608, 0.1333788, 232.6027]\n",
      "[148/250] Losses: [90.46714, 5.829753, 84.63739]\n",
      "[149/250] Losses: [65.04354, 5.4502096, 59.593334]\n",
      "Episode: 084, Reward: 200\n",
      "[150/250] Losses: [340.41446, -3.0011554, 343.41562]\n",
      "[151/250] Losses: [66.04214, 5.28595, 60.756187]\n",
      "[152/250] Losses: [62.24622, 5.0785055, 57.167713]\n",
      "Episode: 085, Reward: 200\n",
      "[153/250] Losses: [460.20746, -6.0264497, 466.23392]\n",
      "[154/250] Losses: [58.44791, 4.750705, 53.697205]\n",
      "[155/250] Losses: [82.51271, 5.904132, 76.60858]\n",
      "Episode: 086, Reward: 200\n",
      "[156/250] Losses: [553.4819, -12.688283, 566.17017]\n",
      "[157/250] Losses: [74.596825, 5.4130926, 69.18373]\n",
      "[158/250] Losses: [55.67411, 5.0708714, 50.60324]\n",
      "Episode: 087, Reward: 200\n",
      "[159/250] Losses: [588.0473, -15.045029, 603.09235]\n",
      "[160/250] Losses: [86.511696, 5.758916, 80.75278]\n",
      "Episode: 088, Reward: 149\n",
      "[161/250] Losses: [642.5308, -16.541494, 659.0723]\n",
      "[162/250] Losses: [75.4998, 5.4659095, 70.03389]\n",
      "[163/250] Losses: [73.65975, 6.0407453, 67.619]\n",
      "[164/250] Losses: [91.704094, 7.032316, 84.671776]\n",
      "Episode: 089, Reward: 200\n",
      "[165/250] Losses: [161.37106, 2.8027062, 158.56836]\n",
      "[166/250] Losses: [58.00618, 5.549917, 52.45626]\n",
      "Episode: 090, Reward: 169\n",
      "[167/250] Losses: [577.5304, -17.366882, 594.8973]\n",
      "[168/250] Losses: [59.88984, 4.3002167, 55.589622]\n",
      "[169/250] Losses: [93.67048, 6.5675077, 87.102974]\n",
      "Episode: 091, Reward: 170\n",
      "[170/250] Losses: [481.24316, -11.493396, 492.73657]\n",
      "[171/250] Losses: [56.562057, 4.1515446, 52.410515]\n",
      "[172/250] Losses: [108.55491, 6.182404, 102.372505]\n",
      "Episode: 092, Reward: 200\n",
      "[173/250] Losses: [665.2502, -10.087416, 675.3376]\n",
      "[174/250] Losses: [71.94896, 4.623512, 67.32545]\n",
      "[175/250] Losses: [73.24826, 4.708243, 68.540016]\n",
      "Episode: 093, Reward: 200\n",
      "[176/250] Losses: [704.0345, -12.760164, 716.7947]\n",
      "[177/250] Losses: [70.63733, 5.3865795, 65.25075]\n",
      "[178/250] Losses: [87.24751, 5.5169883, 81.73052]\n",
      "Episode: 094, Reward: 200\n",
      "[179/250] Losses: [873.6775, -16.0075, 889.685]\n",
      "[180/250] Losses: [77.34368, 4.7180557, 72.625626]\n",
      "[181/250] Losses: [73.156456, 5.948629, 67.207825]\n",
      "Episode: 095, Reward: 200\n",
      "[182/250] Losses: [694.6452, -17.92635, 712.57153]\n",
      "[183/250] Losses: [82.62184, 6.3325405, 76.2893]\n",
      "[184/250] Losses: [79.865295, 5.476822, 74.38847]\n",
      "Episode: 096, Reward: 200\n",
      "[185/250] Losses: [795.9475, -17.62335, 813.57086]\n",
      "[186/250] Losses: [79.75919, 5.5074387, 74.251755]\n",
      "[187/250] Losses: [66.77108, 4.9715443, 61.799534]\n",
      "[188/250] Losses: [70.58248, 5.0337405, 65.548744]\n",
      "Episode: 097, Reward: 200\n",
      "[189/250] Losses: [250.64694, 0.94249403, 249.70445]\n",
      "[190/250] Losses: [52.40373, 4.2623816, 48.141346]\n",
      "[191/250] Losses: [84.07866, 6.0064116, 78.07225]\n",
      "Episode: 098, Reward: 200\n",
      "[192/250] Losses: [424.68948, -4.315342, 429.00482]\n",
      "[193/250] Losses: [65.50078, 4.620175, 60.8806]\n",
      "[194/250] Losses: [68.69736, 5.36892, 63.328434]\n",
      "Episode: 099, Reward: 200\n",
      "[195/250] Losses: [572.3311, -10.178093, 582.5092]\n",
      "[196/250] Losses: [83.45285, 5.801585, 77.65127]\n",
      "Episode: 100, Reward: 127\n",
      "[197/250] Losses: [619.0195, -10.046474, 629.0659]\n",
      "Episode: 101, Reward: 077\n",
      "[198/250] Losses: [633.6715, -10.742562, 644.41406]\n",
      "Episode: 102, Reward: 035\n",
      "Episode: 103, Reward: 052\n",
      "[199/250] Losses: [791.69995, -16.057413, 807.7574]\n",
      "Episode: 104, Reward: 023\n",
      "Episode: 105, Reward: 036\n",
      "[200/250] Losses: [848.97424, -21.054174, 870.02844]\n",
      "[201/250] Losses: [79.99525, 4.8188877, 75.17636]\n",
      "Episode: 106, Reward: 100\n",
      "[202/250] Losses: [419.05148, -5.2107587, 424.26224]\n",
      "Episode: 107, Reward: 087\n",
      "[203/250] Losses: [421.78683, -11.071317, 432.85815]\n",
      "[204/250] Losses: [73.952034, 5.334175, 68.61786]\n",
      "Episode: 108, Reward: 113\n",
      "[205/250] Losses: [313.26575, -8.27417, 321.53992]\n",
      "[206/250] Losses: [87.44685, 5.209895, 82.23696]\n",
      "Episode: 109, Reward: 144\n",
      "[207/250] Losses: [339.27643, -11.708681, 350.9851]\n",
      "[208/250] Losses: [83.80353, 7.005495, 76.798035]\n",
      "[209/250] Losses: [141.20728, 6.7162476, 134.49103]\n",
      "Episode: 110, Reward: 200\n",
      "[210/250] Losses: [406.3407, -11.320166, 417.66086]\n",
      "[211/250] Losses: [84.264, 5.505938, 78.758064]\n",
      "[212/250] Losses: [133.99966, 7.9677424, 126.03192]\n",
      "[213/250] Losses: [72.474304, 5.3568325, 67.11747]\n",
      "Episode: 111, Reward: 200\n",
      "[214/250] Losses: [109.764915, 4.4932346, 105.27168]\n",
      "[215/250] Losses: [76.040825, 4.427266, 71.613556]\n",
      "[216/250] Losses: [100.95823, 6.52549, 94.43274]\n",
      "Episode: 112, Reward: 200\n",
      "[217/250] Losses: [243.1076, -1.2615125, 244.36911]\n",
      "[218/250] Losses: [88.899796, 5.205714, 83.694084]\n",
      "[219/250] Losses: [72.713066, 5.869992, 66.84307]\n",
      "Episode: 113, Reward: 200\n",
      "[220/250] Losses: [360.7955, -5.210384, 366.0059]\n",
      "[221/250] Losses: [73.61987, 4.6328936, 68.98698]\n",
      "[222/250] Losses: [98.59306, 5.7306814, 92.86238]\n",
      "Episode: 114, Reward: 200\n",
      "[223/250] Losses: [464.8598, -5.044347, 469.90414]\n",
      "[224/250] Losses: [69.34749, 4.7803946, 64.56709]\n",
      "[225/250] Losses: [108.41855, 6.4691324, 101.94942]\n",
      "Episode: 115, Reward: 200\n",
      "[226/250] Losses: [555.6419, -11.223145, 566.86505]\n",
      "[227/250] Losses: [74.37987, 4.6526747, 69.727196]\n",
      "Episode: 116, Reward: 100\n",
      "[228/250] Losses: [164.32918, 2.4696004, 161.85957]\n",
      "[229/250] Losses: [75.30067, 4.42974, 70.870926]\n",
      "[230/250] Losses: [84.28117, 5.8832464, 78.39793]\n",
      "Episode: 117, Reward: 200\n",
      "[231/250] Losses: [337.28644, -3.2559745, 340.54242]\n",
      "[232/250] Losses: [63.174618, 4.4604225, 58.714195]\n",
      "[233/250] Losses: [71.56148, 4.5435925, 67.01788]\n",
      "Episode: 118, Reward: 200\n",
      "[234/250] Losses: [462.86407, -5.1961594, 468.06024]\n",
      "[235/250] Losses: [55.7292, 5.608323, 50.120876]\n",
      "[236/250] Losses: [76.304214, 5.8701563, 70.43406]\n",
      "Episode: 119, Reward: 200\n",
      "[237/250] Losses: [601.8847, -11.917534, 613.80225]\n",
      "[238/250] Losses: [55.874123, 5.4604044, 50.41372]\n",
      "[239/250] Losses: [54.71432, 4.8657722, 49.84855]\n",
      "Episode: 120, Reward: 200\n",
      "[240/250] Losses: [596.3255, -15.02588, 611.3514]\n",
      "[241/250] Losses: [53.2733, 4.3743496, 48.89895]\n",
      "[242/250] Losses: [55.2275, 4.4716053, 50.755894]\n",
      "Episode: 121, Reward: 200\n",
      "[243/250] Losses: [570.3766, -14.780272, 585.15686]\n",
      "[244/250] Losses: [42.558086, 3.458346, 39.09974]\n",
      "[245/250] Losses: [60.47418, 4.0223284, 56.45185]\n",
      "Episode: 122, Reward: 200\n",
      "[246/250] Losses: [634.66626, -15.954165, 650.6204]\n",
      "[247/250] Losses: [65.97591, 4.48622, 61.489685]\n",
      "[248/250] Losses: [39.634686, 3.6129518, 36.021732]\n",
      "Episode: 123, Reward: 200\n",
      "[249/250] Losses: [476.1762, -16.936161, 493.11237]\n",
      "[250/250] Losses: [94.99703, 6.3170996, 88.67993]\n",
      "Finished training, testing...\n",
      "200 out of 200\n"
     ]
    }
   ],
   "source": [
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training, testing...\")\n",
    "print(\"%d out of 200\" % agent.test(env)) # 200 out of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
